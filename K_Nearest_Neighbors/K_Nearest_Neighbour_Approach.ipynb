{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I will be loading the data from the cvs files to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = pd.read_csv(\"... path to train.csv folder...\")\n",
    "crops_unknown = pd.read_csv(\"...path to test.csv folder...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing steps:\n",
    "        1) Define the features and the labels and label them as x and y\n",
    "        2) Fill the missing the data. Here I am using the data points before them to fill the missing data. \n",
    "        3) Then the features are transformed to decrease the skeweness of the features.\n",
    "        3) Then the data is scaled so that all the features have the same importance.\n",
    "        4) Then the data points are divided into the training set and evaluation set.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "x = crops.iloc[: , 1:9]\n",
    "y = crops.iloc[: , 9]\n",
    "\n",
    "#2\n",
    "x = x.fillna(method = \"bfill\" , axis = 0)\n",
    "x = np.sqrt(x)\n",
    "\n",
    "#3\n",
    "sc_x = StandardScaler()\n",
    "x = sc_x.fit_transform(x)\n",
    "\n",
    "#4\n",
    "x_train , x_test , y_train , y_test = split(x , y , test_size = 0.2 , stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the imbalanced nature of the data\n",
    "        Here the training data is undersampled according to cluster centroids. \n",
    "        PROS:\n",
    "            1) This makes the data more balanced.\n",
    "            2) This makes the various clusters more dictinct and hence the classifier has a easier job.\n",
    "        CONS:\n",
    "            1) This will remove too many data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids()\n",
    "\n",
    "x_cc, y_cc = cc.fit_sample(x_train , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the K-Nearest-Neighbor model will be defined and trained\n",
    "        \n",
    "GridSeachCV is going to be used for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_neighbors\" : [5] , \n",
    "              \"leaf_size\" : [1 , 3 , 5 , 7 , 9 , 10] , \n",
    "              \"algorithm\" : [\"auto\" , \"kd_tree\"] , \n",
    "              \"n_jobs\" : [-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "model = GridSearchCV(knn , param_grid = parameters)\n",
    "\n",
    "#Training\n",
    "model.fit(x_cc , y_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will be predicting the evaluation set\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model using various metrics\n",
    "\n",
    "The accuracy will be lower, but the confusion matrix shows that the model was somewhat able to predict the minority   class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will be taking a look at the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test , y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will be taking a look at the accuracy score of the model\n",
    "print(model.score(x_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I will be printing the classification report of the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test , y_pred , digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "fileName = \"KNN_with_cluster_undersampling.sav\"\n",
    "joblib.dump(model , fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model and predicting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\"KNN_with_cluster_undersampling.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unknown = crops_unknown.iloc[: , 1:9]\n",
    "crops_id = crops_unknown.iloc[: , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unknown = x_unknown.fillna(method = \"bfill\" , axis = 0)\n",
    "x_unknown = np.sqrt(x_unknown)\n",
    "\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "x_unknown = sc_x.fit_transform(x_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unknown = loaded_model.predict(x_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ID':crops_id, 'Crop_Damage':y_unknown})\n",
    "submission.to_csv('Submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}